import json
from typing import Dict, List, Any
from datetime import datetime


class ReviewFormatter:
    def __init__(self, output_config: Dict):
        self.output_config = output_config
        self.max_comment_length = output_config.get("max_comment_length", 500)
        self.max_summary_length = output_config.get("max_summary_length", 2000)
    
    def format_pr_comment(self, review_results: Dict) -> str:
        """Format comprehensive review results as a PR comment"""
        
        comment_parts = []
        
        # Header
        comment_parts.append("## 🤖 AI Code Review Summary")
        comment_parts.append("")
        
        # Overall metrics
        overall_metrics = self._calculate_overall_metrics(review_results)
        comment_parts.extend(self._format_metrics_section(overall_metrics))
        
        # Critical issues first
        critical_issues = self._extract_critical_issues(review_results)
        if critical_issues:
            comment_parts.extend(self._format_critical_issues_section(critical_issues))
        
        # Security summary
        if review_results.get("security_analysis"):
            comment_parts.extend(self._format_security_section(review_results["security_analysis"]))
        
        # Performance summary
        if review_results.get("performance_analysis"):
            comment_parts.extend(self._format_performance_section(review_results["performance_analysis"]))
        
        # Documentation summary
        if review_results.get("documentation_analysis"):
            comment_parts.extend(self._format_documentation_section(review_results["documentation_analysis"]))
        
        # Top recommendations
        recommendations = self._prioritize_recommendations(review_results)
        if recommendations:
            comment_parts.extend(self._format_recommendations_section(recommendations))
        
        # File-by-file summary
        if self.output_config.get("include_file_summaries", True):
            comment_parts.extend(self._format_file_summaries(review_results.get("file_reviews", [])))
        
        # Footer
        comment_parts.append("")
        comment_parts.append("---")
        comment_parts.append("*Generated by AI Code Review Action*")
        
        full_comment = "\n".join(comment_parts)
        
        # Truncate if too long
        if len(full_comment) > self.max_summary_length:
            full_comment = full_comment[:self.max_summary_length - 100] + "\n\n*[Comment truncated due to length]*"
        
        return full_comment
    
    def format_line_comment(self, issue: Dict, file_path: str) -> str:
        """Format a single issue as a line comment"""
        
        comment_parts = []
        
        # Issue type and severity
        issue_type = issue.get("type", "info").upper()
        severity = issue.get("severity", "low").upper()
        
        if severity in ["CRITICAL", "HIGH"]:
            emoji = "🚨" if severity == "CRITICAL" else "⚠️"
            comment_parts.append(f"{emoji} **{severity} {issue_type}**")
        else:
            emoji = "💡" if issue_type == "SUGGESTION" else "ℹ️"
            comment_parts.append(f"{emoji} **{issue_type}**")
        
        comment_parts.append("")
        
        # Issue description
        description = issue.get("message", "")
        if description:
            comment_parts.append(description)
            comment_parts.append("")
        
        # Suggestion for improvement
        suggestion = issue.get("suggestion", "")
        if suggestion:
            comment_parts.append("**Suggested fix:**")
            comment_parts.append(suggestion)
            comment_parts.append("")
        
        # Code example if available
        code_example = issue.get("code_example", "")
        if code_example:
            comment_parts.append("**Example:**")
            comment_parts.append("```")
            comment_parts.append(code_example)
            comment_parts.append("```")
        
        comment = "\n".join(comment_parts)
        
        # Truncate if too long
        if len(comment) > self.max_comment_length:
            comment = comment[:self.max_comment_length - 50] + "\n\n*[Truncated]*"
        
        return comment
    
    def _calculate_overall_metrics(self, review_results: Dict) -> Dict:
        """Calculate overall review metrics"""
        
        metrics = {
            "files_reviewed": len(review_results.get("file_reviews", [])),
            "total_issues": 0,
            "critical_issues": 0,
            "security_issues": 0,
            "performance_issues": 0,
            "documentation_issues": 0
        }
        
        for file_review in review_results.get("file_reviews", []):
            # Count code review issues
            code_analysis = file_review.get("code_analysis", {})
            issues = code_analysis.get("issues", {})
            if isinstance(issues, dict):
                for severity_level, issue_list in issues.items():
                    metrics["total_issues"] += len(issue_list)
                    if severity_level == "critical":
                        metrics["critical_issues"] += len(issue_list)
            
            # Count security issues
            security_analysis = file_review.get("security_analysis", {})
            vulnerabilities = security_analysis.get("vulnerabilities", [])
            metrics["security_issues"] += len(vulnerabilities)
            
            # Count performance issues
            performance_analysis = file_review.get("performance_analysis", {})
            perf_issues = performance_analysis.get("performance_issues", [])
            metrics["performance_issues"] += len(perf_issues)
            
            # Count documentation issues
            doc_analysis = file_review.get("documentation_analysis", {})
            doc_issues = doc_analysis.get("documentation_issues", [])
            metrics["documentation_issues"] += len(doc_issues)
        
        return metrics
    
    def _format_metrics_section(self, metrics: Dict) -> List[str]:
        """Format the metrics summary section"""
        
        lines = ["### 📊 Review Metrics", ""]
        
        lines.append(f"- **Files Reviewed:** {metrics['files_reviewed']}")
        lines.append(f"- **Total Issues Found:** {metrics['total_issues']}")
        
        if metrics['critical_issues'] > 0:
            lines.append(f"- **🚨 Critical Issues:** {metrics['critical_issues']}")
        
        if metrics['security_issues'] > 0:
            lines.append(f"- **🔒 Security Issues:** {metrics['security_issues']}")
        
        if metrics['performance_issues'] > 0:
            lines.append(f"- **⚡ Performance Issues:** {metrics['performance_issues']}")
        
        if metrics['documentation_issues'] > 0:
            lines.append(f"- **📝 Documentation Issues:** {metrics['documentation_issues']}")
        
        lines.append("")
        return lines
    
    def _extract_critical_issues(self, review_results: Dict) -> List[Dict]:
        """Extract all critical issues from review results"""
        
        critical_issues = []
        
        for file_review in review_results.get("file_reviews", []):
            file_path = file_review.get("file_path", "unknown")
            
            # Critical code issues
            code_analysis = file_review.get("code_analysis", {})
            critical_code_issues = code_analysis.get("issues", {}).get("critical", [])
            for issue in critical_code_issues:
                issue["file_path"] = file_path
                issue["category"] = "code"
                critical_issues.append(issue)
            
            # Critical security issues
            security_analysis = file_review.get("security_analysis", {})
            critical_security = [
                vuln for vuln in security_analysis.get("vulnerabilities", [])
                if vuln.get("severity") == "critical"
            ]
            for vuln in critical_security:
                vuln["file_path"] = file_path
                vuln["category"] = "security"
                critical_issues.append(vuln)
            
            # Critical performance issues
            performance_analysis = file_review.get("performance_analysis", {})
            critical_performance = [
                issue for issue in performance_analysis.get("performance_issues", [])
                if issue.get("severity") == "critical"
            ]
            for issue in critical_performance:
                issue["file_path"] = file_path
                issue["category"] = "performance"
                critical_issues.append(issue)
        
        return critical_issues
    
    def _format_critical_issues_section(self, critical_issues: List[Dict]) -> List[str]:
        """Format critical issues section"""
        
        lines = ["### 🚨 Critical Issues", ""]
        
        if not critical_issues:
            lines.append("No critical issues found! 🎉")
            lines.append("")
            return lines
        
        for issue in critical_issues[:5]:  # Limit to top 5
            file_path = issue.get("file_path", "unknown")
            category = issue.get("category", "unknown")
            description = issue.get("description", issue.get("message", "No description"))
            
            lines.append(f"- **{category.title()}** in `{file_path}`: {description}")
        
        if len(critical_issues) > 5:
            lines.append(f"- *...and {len(critical_issues) - 5} more critical issues*")
        
        lines.append("")
        return lines
    
    def _format_security_section(self, security_analysis: Dict) -> List[str]:
        """Format security analysis section"""
        
        lines = ["### 🔒 Security Analysis", ""]
        
        summary = security_analysis.get("summary", {})
        total_vulns = summary.get("total_vulnerabilities", 0)
        
        if total_vulns == 0:
            lines.append("✅ No security vulnerabilities detected")
        else:
            severity_breakdown = summary.get("severity_breakdown", {})
            lines.append(f"**Found {total_vulns} potential security issues:**")
            
            for severity in ["critical", "high", "medium", "low"]:
                count = severity_breakdown.get(severity, 0)
                if count > 0:
                    emoji = {"critical": "🚨", "high": "⚠️", "medium": "🔶", "low": "🔹"}.get(severity, "")
                    lines.append(f"  - {emoji} {severity.title()}: {count}")
        
        lines.append("")
        return lines
    
    def _format_performance_section(self, performance_analysis: Dict) -> List[str]:
        """Format performance analysis section"""
        
        lines = ["### ⚡ Performance Analysis", ""]
        
        summary = performance_analysis.get("summary", {})
        total_issues = summary.get("total_performance_issues", 0)
        
        if total_issues == 0:
            lines.append("✅ No significant performance issues detected")
        else:
            lines.append(f"**Found {total_issues} performance optimization opportunities**")
            
            # Highlight high-impact optimizations
            high_impact = summary.get("high_impact_optimizations", 0)
            if high_impact > 0:
                lines.append(f"  - 🎯 {high_impact} high-impact, low-effort optimizations available")
        
        lines.append("")
        return lines
    
    def _format_documentation_section(self, documentation_analysis: Dict) -> List[str]:
        """Format documentation analysis section"""
        
        lines = ["### 📝 Documentation Review", ""]
        
        summary = documentation_analysis.get("summary", {})
        avg_coverage = summary.get("average_coverage", {})
        
        if avg_coverage:
            functions_avg = avg_coverage.get("functions_avg", "0%")
            lines.append(f"**Documentation Coverage:** {functions_avg} of functions documented")
        
        total_issues = summary.get("total_documentation_issues", 0)
        if total_issues > 0:
            lines.append(f"**Issues Found:** {total_issues} documentation improvements needed")
        else:
            lines.append("✅ Documentation quality looks good")
        
        lines.append("")
        return lines
    
    def _prioritize_recommendations(self, review_results: Dict) -> List[str]:
        """Extract and prioritize top recommendations"""
        
        all_recommendations = []
        
        # Collect recommendations from all analyses
        for file_review in review_results.get("file_reviews", []):
            code_analysis = file_review.get("code_analysis", {})
            all_recommendations.extend(code_analysis.get("recommendations", []))
        
        # Add summary-level recommendations
        for analysis_type in ["security_analysis", "performance_analysis", "documentation_analysis"]:
            analysis = review_results.get(analysis_type, {})
            next_steps = analysis.get("next_steps", [])
            all_recommendations.extend(next_steps[:3])  # Top 3 from each
        
        # Deduplicate and prioritize
        unique_recommendations = list(dict.fromkeys(all_recommendations))
        
        return unique_recommendations[:5]  # Top 5 recommendations
    
    def _format_recommendations_section(self, recommendations: List[str]) -> List[str]:
        """Format recommendations section"""
        
        lines = ["### 💡 Top Recommendations", ""]
        
        for i, rec in enumerate(recommendations, 1):
            lines.append(f"{i}. {rec}")
        
        lines.append("")
        return lines
    
    def _format_file_summaries(self, file_reviews: List[Dict]) -> List[str]:
        """Format file-by-file summaries"""
        
        if not file_reviews:
            return []
        
        lines = ["### 📁 File-by-File Summary", ""]
        lines.append("<details>")
        lines.append("<summary>Click to expand file details</summary>")
        lines.append("")
        
        for file_review in file_reviews:
            file_path = file_review.get("file_path", "unknown")
            code_analysis = file_review.get("code_analysis", {})
            
            lines.append(f"#### `{file_path}`")
            
            # Overall quality score
            quality_score = code_analysis.get("overall_quality", "5")
            lines.append(f"**Quality Score:** {quality_score}/10")
            
            # Issue summary
            issues = code_analysis.get("issues", {})
            if isinstance(issues, dict):
                total_file_issues = sum(len(issue_list) for issue_list in issues.values())
                if total_file_issues > 0:
                    lines.append(f"**Issues:** {total_file_issues} found")
                else:
                    lines.append("**Issues:** None found ✅")
            
            lines.append("")
        
        lines.append("</details>")
        lines.append("")
        
        return lines
    
    def format_github_outputs(self, review_results: Dict) -> Dict[str, str]:
        """Format results for GitHub Action outputs"""
        
        metrics = self._calculate_overall_metrics(review_results)
        
        # Create summary
        summary_parts = [
            f"Reviewed {metrics['files_reviewed']} files",
            f"Found {metrics['total_issues']} total issues"
        ]
        
        if metrics['critical_issues'] > 0:
            summary_parts.append(f"{metrics['critical_issues']} critical")
        
        summary = ", ".join(summary_parts)
        
        # Top recommendations
        recommendations = self._prioritize_recommendations(review_results)
        recommendations_text = "; ".join(recommendations[:3]) if recommendations else "No major issues found"
        
        return {
            "review_summary": summary,
            "issues_found": str(metrics['total_issues']),
            "recommendations": recommendations_text
        }